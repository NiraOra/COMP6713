{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUT stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Compare and analyse the output of NLTK and spaCy POS taggers for the following sentences:\n",
    "1. The cat is on the mat\n",
    "2.  \"Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\" is a grammatically correct English sentence. It is often used as a stress test for POS taggers. [1]\n",
    "3. “The old man the boat”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from en-core-web-sm==3.2.0) (3.2.6)\n",
      "Requirement already satisfied: setuptools in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (67.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/niranjanaarunmenon/.local/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.66.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.10)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (6.4.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.7.4.1 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.12)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.32.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.10)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.11.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (24.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.1.2)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.2.0)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.2.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.2.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('cat', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mat', 'NN')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The cat is on the mat\"\n",
    "tokenized_sent = word_tokenize(sentence)\n",
    "tokenized_sent\n",
    "result = nltk.pos_tag(tokenized_sent)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:434.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DET\n",
      "cat NOUN\n",
      "is AUX\n",
      "on ADP\n",
      "the DET\n",
      "mat NOUN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sentence)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2 Masked language modelling (MLM) is a form of self-supervision. A dataset of unlabeled sentences can be converted to a self-supervised MLM task as follows: Randomly mask a word in a sentence. The input to the language model is the sentence with the word masked, the output is the word at the masked position in the original sentence.\n",
    "\n",
    "The sentences in the dataset are:\n",
    "1.\tYou can never be overdressed or overeducated.\n",
    "2.\tAlways forgive your enemies; nothing annoys them so much.\n",
    "3.\tI am not young enough to know everything.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***(A)\tDraw the black-box view to create a self-supervised MLM task as described above.***\n",
    "\n",
    "(i) You can never be ____ or overeducated --> overdressed\n",
    "(ii)Always forgive your ___; nothing annoys them so much. -> enemies\n",
    "(iii) I am not ____ enough to know everything. -> young"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(B)\tUse the HuggingFace pipeline ‘fill-mask’ to obtain the top 5 words for each of the sentences above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niranjanaarunmenon/anaconda3/envs/COMP3900/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "masklm = pipeline(\"fill-mask\", model=\"roberta-base\", top_k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by ()\n",
    "\n",
    "(i) [{'score': 0.04341038689017296,\n",
    "  'token': 5125,\n",
    "  'token_str': ' defeated',\n",
    "  'sequence': 'You can never be defeated or overeducated.'},\n",
    " {'score': 0.039416518062353134,\n",
    "  'token': 16658,\n",
    "  'token_str': ' depressed',\n",
    "  'sequence': 'You can never be depressed or overeducated.'},\n",
    " {'score': 0.031338050961494446,\n",
    "  'token': 17067,\n",
    "  'token_str': ' exhausted',\n",
    "  'sequence': 'You can never be exhausted or overeducated.'},\n",
    " {'score': 0.03050517849624157,\n",
    "  'token': 23809,\n",
    "  'token_str': ' bored',\n",
    "  'sequence': 'You can never be bored or overeducated.'},\n",
    " {'score': 0.024364957585930824,\n",
    "  'token': 4875,\n",
    "  'token_str': ' controlled',\n",
    "  'sequence': 'You can never be controlled or overeducated.'}]\n",
    "\n",
    "(ii) [{'score': 0.28885093331336975,\n",
    "  'token': 1041,\n",
    "  'token_str': ' parents',\n",
    "  'sequence': 'Always forgive your parents; nothing annoys them so much.'},\n",
    " {'score': 0.1554063856601715,\n",
    "  'token': 408,\n",
    "  'token_str': ' children',\n",
    "  'sequence': 'Always forgive your children; nothing annoys them so much.'},\n",
    " {'score': 0.11546684056520462,\n",
    "  'token': 11058,\n",
    "  'token_str': ' enemies',\n",
    "  'sequence': 'Always forgive your enemies; nothing annoys them so much.'},\n",
    " {'score': 0.07019894570112228,\n",
    "  'token': 964,\n",
    "  'token_str': ' friends',\n",
    "  'sequence': 'Always forgive your friends; nothing annoys them so much.'},\n",
    " {'score': 0.036020953208208084,\n",
    "  'token': 6774,\n",
    "  'token_str': ' relatives',\n",
    "  'sequence': 'Always forgive your relatives; nothing annoys them so much.'}]\n",
    "\n",
    "(iii) [{'score': 0.3585122227668762,\n",
    "  'token': 2793,\n",
    "  'token_str': ' smart',\n",
    "  'sequence': 'I am not smart enough to know everything.'},\n",
    " {'score': 0.296065092086792,\n",
    "  'token': 793,\n",
    "  'token_str': ' old',\n",
    "  'sequence': 'I am not old enough to know everything.'},\n",
    " {'score': 0.056942977011203766,\n",
    "  'token': 11036,\n",
    "  'token_str': ' wise',\n",
    "  'sequence': 'I am not wise enough to know everything.'},\n",
    " {'score': 0.050608664751052856,\n",
    "  'token': 12038,\n",
    "  'token_str': ' intelligent',\n",
    "  'sequence': 'I am not intelligent enough to know everything.'},\n",
    " {'score': 0.018215401098132133,\n",
    "  'token': 2984,\n",
    "  'token_str': ' experienced',\n",
    "  'sequence': 'I am not experienced enough to know everything.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3585122227668762,\n",
       "  'token': 2793,\n",
       "  'token_str': ' smart',\n",
       "  'sequence': 'I am not smart enough to know everything.'},\n",
       " {'score': 0.296065092086792,\n",
       "  'token': 793,\n",
       "  'token_str': ' old',\n",
       "  'sequence': 'I am not old enough to know everything.'},\n",
       " {'score': 0.056942977011203766,\n",
       "  'token': 11036,\n",
       "  'token_str': ' wise',\n",
       "  'sequence': 'I am not wise enough to know everything.'},\n",
       " {'score': 0.050608664751052856,\n",
       "  'token': 12038,\n",
       "  'token_str': ' intelligent',\n",
       "  'sequence': 'I am not intelligent enough to know everything.'},\n",
       " {'score': 0.018215401098132133,\n",
       "  'token': 2984,\n",
       "  'token_str': ' experienced',\n",
       "  'sequence': 'I am not experienced enough to know everything.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = masklm(\"I am not <mask> enough to know everything.\")\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C)\tCalculate precision@5. Recall from the lecture that precision is the number of correctly retrieved words. “@5” indicates that any of the top 5 words need to be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q. 3. Use the spaCy matcher code provided in the class and extend it to also return the WordNet lemmas of words that are extracted as entities.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Bank of New York ADR Index, which tracks depositary receipts traded on major U.S. stock exchanges, gained 1.3% to 183.32 points in recent session. The index lost 4.63 from the beginning of July. American Depositary Receipts are dollar-denominated securities that are traded in the U.S. but represent ownership of shares in a non-U.S. company.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are:\n",
      "The Bank of New York ADR Index, which tracks depositary receipts traded on major U.S. stock exchanges, gained 1.3% to 183.32 points in recent session.\n",
      "The index lost 4.63 from the beginning of July.\n",
      "American Depositary Receipts are dollar-denominated securities that are traded in the U.S. but represent ownership of shares in a non-U.S. company.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "print(\"Sentences are:\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***extract entities***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG  |  The Bank of New York\n",
      "ORG  |  ADR\n",
      "GPE  |  U.S.\n",
      "PERCENT  |  1.3%\n",
      "CARDINAL  |  183.32\n",
      "CARDINAL  |  4.63\n",
      "DATE  |  the beginning of July\n",
      "NORP  |  American\n",
      "GPE  |  U.S.\n",
      "GPE  |  non-U.S.\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.label_, ' | ', entity.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_lemma(word):\n",
    "    return lemmatizer.lemmatize(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_lemmas = {}\n",
    "for entity in doc.ents:\n",
    "    words = entity.text.split()\n",
    "    lemmas = [get_wordnet_lemma(word) for word in words]\n",
    "    entity_lemmas[entity.text] = {\"label\": entity.label_, \"lemmas\": lemmas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: The Bank of New York, Label: ORG, Lemmas: ['the', 'bank', 'of', 'new', 'york']\n",
      "Entity: ADR, Label: ORG, Lemmas: ['adr']\n",
      "Entity: U.S., Label: GPE, Lemmas: ['u.s.']\n",
      "Entity: 1.3%, Label: PERCENT, Lemmas: ['1.3%']\n",
      "Entity: 183.32, Label: CARDINAL, Lemmas: ['183.32']\n",
      "Entity: 4.63, Label: CARDINAL, Lemmas: ['4.63']\n",
      "Entity: the beginning of July, Label: DATE, Lemmas: ['the', 'beginning', 'of', 'july']\n",
      "Entity: American, Label: NORP, Lemmas: ['american']\n",
      "Entity: non-U.S., Label: GPE, Lemmas: ['non-u.s.']\n"
     ]
    }
   ],
   "source": [
    "for entity, data in entity_lemmas.items():\n",
    "    print(f\"Entity: {entity}, Label: {data['label']}, Lemmas: {data['lemmas']}\")\n",
    "    \n",
    "# also there are dict of stopwords\n",
    "# filtered_entity_lemmas: filtering based on stop_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP3900",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
